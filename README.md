# Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation (ACM MM 2023)
This repository is the official implementation for ACM MM 2023 paper "Online Distillation-enhanced Multi-modal Transformer for Sequential Recommendation".
## Brief Introduction
The paper focuses on multi-modal recommendation systems, which integrate various types of information. While traditional collaborative filtering-based multi-modal recommendation systems have received significant attention, research on multi-modal sequential recommendation is still in its early stages. We investigate the importance of item representation learning and information fusion from heterogeneous data sources, and propose a new model-agnostic framework called "**Online Distillation-enhanced Multi-modal Transformer (ODMT)**" to enhance feature interaction and mutual learning among multi-source input (ID, text, and image) while improving recommendation accuracy. The framework includes an ID-aware Multi-modal Transformer module for information interaction and an online distillation training strategy to improve prediction robustness. Empirical experiments on video content and e-commerce recommendation datasets show that the proposed framework achieves approximately 10% performance improvement compared to baseline models.
## News
The code will be released soon.
<!-- # Citation
If you find our project useful in your research, please consider citing:
-->
